{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreNasci/ECO904/blob/main/ECO904_05_Supervisionado_Regressivo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base de Dados Regressivo"
      ],
      "metadata": {
        "id": "6LXu3mmyO0wS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[\n",
        "Computer Hardware Data Set](http://archive.ics.uci.edu/ml/datasets/Computer+Hardware)"
      ],
      "metadata": {
        "id": "vsh1znWFQxZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attribute Information:**\n",
        "\n",
        "1. vendor name: 30\n",
        "(adviser, amdahl,apollo, basf, bti, burroughs, c.r.d, cambex, cdc, dec,\n",
        "dg, formation, four-phase, gould, honeywell, hp, ibm, ipl, magnuson,\n",
        "microdata, nas, ncr, nixdorf, perkin-elmer, prime, siemens, sperry,\n",
        "sratus, wang)\n",
        "2. Model Name: many unique symbols\n",
        "3. MYCT: machine cycle time in nanoseconds (integer)\n",
        "4. MMIN: minimum main memory in kilobytes (integer)\n",
        "5. MMAX: maximum main memory in kilobytes (integer)\n",
        "6. CACH: cache memory in kilobytes (integer)\n",
        "7. CHMIN: minimum channels in units (integer)\n",
        "8. CHMAX: maximum channels in units (integer)\n",
        "9. PRP: published relative performance (integer)\n",
        "10. ERP: estimated relative performance from the original article (integer)"
      ],
      "metadata": {
        "id": "uTFMDVEvRtJi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p-ge-PgOvd5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/cpu-performance/machine.data'\n",
        "\n",
        "hardware = pd.read_csv(url,names=['vendor','model','MYCT','MMIN','MMAX','CACH','CHMIN','CHMAX','PRP','ERP'])\n",
        "hardware"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisando Dados"
      ],
      "metadata": {
        "id": "g9mx_UICMxyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colunas desnecessarias - não tem finalidade no treinamento\n",
        "hardware = hardware.drop(['vendor','model'],axis=1)\n",
        "hardware"
      ],
      "metadata": {
        "id": "O5PY9ufyMwue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hardware.describe().T # taxas de variação completamente diferentes umas das outras\n",
        "# trabalhar com escalas diferentes é ruim (escalas de valores) = necessita pré-processamento"
      ],
      "metadata": {
        "id": "Q8knKFL67rQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "g = sns.heatmap(abs(hardware.corr()),annot=True)\n",
        "# correlação entre as colunas\n",
        "# foi feito o absoluto da correlação, pois o que importa é o grau de correlação\n",
        "# e não se ela é positiva ou negativa "
      ],
      "metadata": {
        "id": "kvkRY9i00HFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eliminar colunas com alto grau de correlação? # fazer isso após rodar todas as células \n",
        "# remover colunas com mais de 80% de correlação\n",
        "# remover = [] # remover esse comentário dapós chegar ao fim do documento \n",
        "# inserir colunas e rodar de novo, como PRP\n",
        "# hardware = hardware.drop(remover,axis=1)\n",
        "# hardware"
      ],
      "metadata": {
        "id": "tfkBAOyXNcc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# não possui distribuição gaussiana\n",
        "g = hardware.hist()"
      ],
      "metadata": {
        "id": "JC79vbQW011e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# foco especial no tutor\n",
        "_ = hardware['ERP'].hist()"
      ],
      "metadata": {
        "id": "VMlNTe0wShJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uso de autovetores\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X = hardware[hardware.columns[:-1]].values\n",
        "y = hardware[hardware.columns[-1]].values\n",
        "\n",
        "Xp = PCA(n_components=2,random_state=42).fit_transform(X)\n",
        "\n",
        "_ = sns.scatterplot(x=Xp[:,0],y=Xp[:,1],hue=y)\n",
        "# o seaborn não é capaz de plotar tudo, apenas alguns steps para se \n",
        "# ter uma noção da concentração dos pontos"
      ],
      "metadata": {
        "id": "im3s-FBUR4Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avaliação de Aprendizados Regressivos"
      ],
      "metadata": {
        "id": "kG9Lx63OT_7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost # necessário ser instalado"
      ],
      "metadata": {
        "id": "-YdVrpZefiDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import (dummy, ensemble, gaussian_process, kernel_ridge, \n",
        "                     linear_model, metrics, model_selection, neighbors, \n",
        "                     neural_network, svm, tree, preprocessing, pipeline)\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from statistics import mean \n",
        "from tqdm.notebook import tqdm # informa o progresso \n",
        "import warnings # remove os warnings\n",
        "\n",
        "preprocessador = [ # pré-processadores de ajuste de escala\n",
        "    None,\n",
        "    preprocessing.MinMaxScaler(),\n",
        "    preprocessing.MaxAbsScaler(),\n",
        "    preprocessing.Normalizer(),\n",
        "    preprocessing.RobustScaler(),\n",
        "    preprocessing.StandardScaler(),\n",
        "]\n",
        "\n",
        "aprendizados = [ # técnicas de aprendizado\n",
        "    dummy.DummyRegressor(),\n",
        "    linear_model.LinearRegression(),\n",
        "    linear_model.Ridge(random_state=42),\n",
        "    linear_model.SGDRegressor(random_state=42),\n",
        "    linear_model.ElasticNet(random_state=42),\n",
        "    ensemble.AdaBoostRegressor(random_state=42, max_depth=5), # arvore\n",
        "    ensemble.BaggingRegressor(random_state=42, max_depth=5), #  arvore\n",
        "    ensemble.ExtraTreesRegressor(random_state=42, max_depth=5), # arvore\n",
        "    ensemble.GradientBoostingRegressor(random_state=42, max_depth=5), # arvore\n",
        "    ensemble.RandomForestRegressor(random_state=42, max_depth=5),#  arvore\n",
        "    ensemble.HistGradientBoostingRegressor(random_state=42, max_depth=5), # arvore\n",
        "    gaussian_process.GaussianProcessRegressor(random_state=42),\n",
        "    kernel_ridge.KernelRidge(),\n",
        "    neighbors.KNeighborsRegressor(),\n",
        "    neighbors.RadiusNeighborsRegressor(),\n",
        "    neural_network.MLPRegressor(random_state=42),\n",
        "    svm.LinearSVR(random_state=42), # regressor do support vector machine linear\n",
        "    svm.NuSVR(),\n",
        "    svm.SVR(),\n",
        "    tree.DecisionTreeRegressor(random_state=42, max_depth=5), # arvore\n",
        "    tree.ExtraTreeClassifier(random_state=42, max_depth=5), # arvore \n",
        "    XGBRegressor(random_state=42, max_depth=5), # arvore \n",
        "    LGBMRegressor(random_state=42, max_depth=5), # arvore\n",
        "    CatBoostRegressor(verbose=0), # verbose != 0 -> imprime o funcionamento\n",
        "]\n",
        "\n",
        "# https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n",
        "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
        "metricas = ['max_error','neg_mean_absolute_error','neg_mean_squared_error','neg_mean_absolute_percentage_error','r2']\n",
        "nomes = ['ME','MAE','MSE','MAPE','R2']\n",
        "# ME = max error, maior erro encontrado na lista de erro (valor absoluto)\n",
        "#   importante para identificar a presença de outliers\n",
        "# MAE = erro médio absoluto = na mesma escala do tutor. Usado de base para\n",
        "#   calcular outros erros (nesse caso, a escala é de 15-1280)\n",
        "# MSE = erro médio quadrático: resolve o problema de sinais (todos os valores\n",
        "#   serão positivos), e também destaca os outliers (pois está elevado a 2)\n",
        "#   valores de outliers explodem\n",
        "# MAPE = erro médio absoluto percentual: transforma-se a escala inicial (15-1280)\n",
        "#   para uma escala de 0-100. Mas MAPE não é limitado a 100, ele pode tender a \n",
        "#   infinito. Isso se dá porque o valor é um float. Quanto menor melhor.\n",
        "# R2 = quase uma acurácia. Quando MAPE == 0, R2 == 1. Quando MAPE == 1, R2 == 0.\n",
        "#   Quando MAPE tende a inf, R2 tende a -inf. Quanto maior R2 melhor\n",
        "# Contudo, MAPE e R2 não são completamente relacionados\n",
        "# R2 é a mátrica padrão no sklearn do aprendizado supervisionado regressivo\n",
        "\n",
        "sinal = [-1,-1,-1,-1,1]\n",
        "\n",
        "pipes = []\n",
        "# monta-se primeiro os pipelines e depois os treina\n",
        "for p in preprocessador:\n",
        "  for a in aprendizados:\n",
        "    if (p != None):\n",
        "      pipes.append(pipeline.make_pipeline(p,a))\n",
        "    else:\n",
        "      pipes.append(pipeline.make_pipeline(a))\n",
        "\n",
        "warnings.filterwarnings('ignore') # desabilita os wanings\n",
        "res = []\n",
        "# treinamento de cada pipeline\n",
        "for p in tqdm(pipes,desc='Pipelines'):\n",
        "\n",
        "  cv_results = model_selection.cross_validate(p,X,y,cv=3,scoring=metricas)\n",
        "  dn = {\n",
        "      'nome':'.'.join([s[0] for s in p.steps]),\n",
        "      'fitTime':mean(cv_results['fit_time']),\n",
        "  }\n",
        "\n",
        "  for m,n,s in zip(metricas,nomes,sinal):\n",
        "    dn[n] = s * mean(cv_results['test_' + m])\n",
        "  res.append(dn)\n",
        "\n",
        "rDF = pd.DataFrame(res)\n",
        "\n",
        "warnings.filterwarnings('default') # restaura warnings pro padrão"
      ],
      "metadata": {
        "id": "PhAaM5JrUCXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 melhores, de acordo com MAPE (% de erro)\n",
        "rDF.sort_values('MAPE',ascending=True).head()"
      ],
      "metadata": {
        "id": "mvBrpWMgSQYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 melhores, de acordo com R2 (% de acerto)\n",
        "rDF.sort_values('R2',ascending=False).tail()"
      ],
      "metadata": {
        "id": "pN05baMn78Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ajustando Técnica"
      ],
      "metadata": {
        "id": "wBNxqnS-3g2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seleciona na coluna nome, todas as linhas que tiverem m1pregressor no nome\n",
        "rDF[rDF['nome'].str.contains('mlpregressor')].sort_values('R2',ascending=False)"
      ],
      "metadata": {
        "id": "Dbxkk9bZ29l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import randint, uniform as randfloat\n",
        "# parâmetros de ajuste do modelo:\n",
        "'''\n",
        "hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', \n",
        "learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, \n",
        "tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n",
        "validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000\n",
        "'''\n",
        "clf = neural_network.MLPRegressor(random_state=42)\n",
        "\n",
        "ajustes = {\n",
        "    'hidden_layer_sizes': [(5,5,),(10,10,),(20,20,),(40,40,)], # regressivos = 1 único reurônio de saída\n",
        "    #'alpha': randfloat(), # só funciona quando usamos fx sigmoidal\n",
        "    'learning_rate_init': randfloat(),\n",
        "    'max_iter':randint(200,1000), # 200 a 1000 iterações com o solver ADAM (vai sortear um valor entre 200 e 1000)\n",
        "    # usa o MAPE negativo para identificar o melhor modelo\n",
        "    # número total de iterações por vez rodada será max_iter x n_iter\n",
        "\n",
        "}\n",
        "\n",
        "rs = model_selection.RandomizedSearchCV(clf, ajustes, cv=3, # número de iterações para poder atingir o ponto ótimo\n",
        "                                        scoring=['neg_mean_absolute_percentage_error','r2'],\n",
        "                                        refit='neg_mean_absolute_percentage_error', \n",
        "                                        n_iter=200, random_state=42, verbose=0) #n~ de iterações do cross validation\n",
        "\n",
        "#warnings.filterwarnings('ignore')\n",
        "rs.fit(X,y)\n",
        "#warnings.filterwarnings('default')\n",
        "\n",
        "rs.best_score_, rs.best_params_"
      ],
      "metadata": {
        "id": "SEGjMIDs9rY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados = {\n",
        "    'fit_time':rs.cv_results_['mean_fit_time'],\n",
        "    'parametros':rs.cv_results_['params'],\n",
        "    'MAPE':-rs.cv_results_['mean_test_neg_mean_absolute_percentage_error'],\n",
        "    'R2':rs.cv_results_['mean_test_r2'],\n",
        "    }\n",
        "pd.DataFrame(dados).sort_values('R2',ascending=False).head()\n",
        "# comparar valores de R2 e MAPE"
      ],
      "metadata": {
        "id": "Cp5eezj6Qc1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# melhor estimador baseado no valor de MAPE (ajustes do estimador)\n",
        "rs.best_estimator_"
      ],
      "metadata": {
        "id": "WZgGvtDb6r2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliando Modelos"
      ],
      "metadata": {
        "id": "DDb6isIiBMR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import PredictionErrorDisplay\n",
        "\n",
        "clf = rs.best_estimator_\n",
        "print(str(clf))\n",
        "y_pred = clf.predict(X)\n",
        "\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n",
        "PredictionErrorDisplay.from_predictions(\n",
        "    y,\n",
        "    y_pred=y_pred,\n",
        "    kind=\"actual_vs_predicted\",\n",
        "    subsample=100,\n",
        "    ax=axs[0],\n",
        "    random_state=0,\n",
        ")\n",
        "axs[0].set_title(\"Actual vs. Predicted values\")\n",
        "PredictionErrorDisplay.from_predictions(\n",
        "    y,\n",
        "    y_pred=y_pred,\n",
        "    kind=\"residual_vs_predicted\",\n",
        "    subsample=100,\n",
        "    ax=axs[1],\n",
        "    random_state=0,\n",
        ")\n",
        "axs[1].set_title(\"Residuals vs. Predicted Values\")\n",
        "_ = fig.suptitle(\"Plotting cross-validated predictions\")\n",
        "# visualização gráfica do erro"
      ],
      "metadata": {
        "id": "ga8P74-1A6KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rDF.sort_values('MAPE',ascending=True).head()"
      ],
      "metadata": {
        "id": "k1lBTuANR0h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "id_best = rDF.sort_values('MAPE',ascending=True).index[0]\n",
        "clf= pipes[id_best]\n",
        "print(str(clf))\n",
        "y_pred =  cross_val_predict(clf, X, y, cv=10)\n",
        "\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(8, 4))\n",
        "PredictionErrorDisplay.from_predictions(\n",
        "    y,\n",
        "    y_pred=y_pred,\n",
        "    kind=\"actual_vs_predicted\",\n",
        "    subsample=100,\n",
        "    ax=axs[0],\n",
        "    random_state=0,\n",
        ")\n",
        "axs[0].set_title(\"Actual vs. Predicted values\")\n",
        "PredictionErrorDisplay.from_predictions(\n",
        "    y,\n",
        "    y_pred=y_pred,\n",
        "    kind=\"residual_vs_predicted\",\n",
        "    subsample=100,\n",
        "    ax=axs[1],\n",
        "    random_state=0,\n",
        ")\n",
        "axs[1].set_title(\"Residuals vs. Predicted Values\")\n",
        "_ = fig.suptitle(\"Plotting cross-validated predictions\")\n",
        "# observar como o modelo concentra mais os pontos sobre a reta em comparação\n",
        "# com o anterior "
      ],
      "metadata": {
        "id": "8_RBiEQTBZ4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avaliando Árvores de Decisão"
      ],
      "metadata": {
        "id": "buIJJBg9RsJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# árvores tem o maior rendimento entre as técnicas clássicas\n",
        "# perde pra deep learning, embora seu consumo energético seja significativamente menor\n",
        "from sklearn.tree import export_graphviz # biblioteca vetorial do python (cria uma representação gráfica)\n",
        "from pydotplus import graph_from_dot_data\n",
        "from IPython.display import Image\n",
        "\n",
        "def ImprimeArvore(tree):\n",
        "  dot_data = export_graphviz(tree, feature_names = hardware.columns[:-1],\n",
        "      out_file=None, filled=True, rounded=True,\n",
        "      special_characters=True,\n",
        "      proportion=False, impurity=False,\n",
        "  )\n",
        "\n",
        "  graph = graph_from_dot_data(dot_data)\n",
        "  png = graph.create_png()\n",
        "  display(Image(png))"
      ],
      "metadata": {
        "id": "h2geZ3ZmViYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mostra os melhores modelos\n",
        "rDF[rDF['nome'].str.contains('randomforestregressor')].sort_values('R2',ascending=False)"
      ],
      "metadata": {
        "id": "DTQJOTI4VbDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 43 minmaxscaler.decisiontreeregressor\n",
        "pp = pipes[43] # buscando o melhor modelo\n",
        "\n",
        "arvore = pp.steps[1][1] # primeiro elemento é o nome da técnica, segundo é o modelo\n",
        "\n",
        "# treinando novamente o modelo, porque quando é feito no gridSearch ele apenas\n",
        "# treina o modelo, mas não o retorna\n",
        "arvore.fit(X,y)\n",
        "imprimeArvore(arvore)\n",
        "# analisar depth da árvore "
      ],
      "metadata": {
        "id": "mCQdRmCB3Wxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# repetir esse procedimento para outras árvores\n",
        "tree = pipes[96].steps[1][1]\n",
        "tree.fit(X,y)\n",
        "\n",
        "ImprimeArvore(tree.estimators_[30])"
      ],
      "metadata": {
        "id": "yHgeDcF0Vtz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp = pipes[105] # floresta\n",
        "floresta = pp.steps[1][1]\n",
        "floresta.fit(X,y)\n",
        "len(floresta.estimators_) # quantidade de arvores na floresta\n",
        "# cada árvore se especializa em uma parte da tabela, o que, em geral,\n",
        "# dá um resultado melhor do que árvores sozinhas "
      ],
      "metadata": {
        "id": "bUof96C25cZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# conferindo uma das árvores\n",
        "arvore = floresta.estimators_[39]\n",
        "ImprimeArvore(arvore)\n",
        "# note que o random_state é o mesmo para as 100 arvores porque foi setado 42 anteriormente\n",
        "arvore = floresta.estimators_[58]\n",
        "ImprimeArvore(arvore)\n",
        "# cada uma das árvores é treinada com um pacote de dados diferentes\n",
        "# a fim de gerar a especialização"
      ],
      "metadata": {
        "id": "YmejK0m658Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rDF[rDF['nome'].str.contains('decisiontreeregressor')].sort_values('R2',ascending=False)"
      ],
      "metadata": {
        "id": "vVHHYa86RvCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y9_L2CnjeSD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atividade"
      ],
      "metadata": {
        "id": "6IZ8U7-1EN5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refaça os passos anteriores com o dataset abaixo:"
      ],
      "metadata": {
        "id": "pFhPOntAEQLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# volume de tráfico de pessoas do metrô interestatudal \n",
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz'\n",
        "\n",
        "metro = pd.read_csv(url, parse_dates=['date_time']) # colab é capaz de ler um arquivo .gz diretamente\n",
        "# e fazer o typecast de date_time automaticamente na leitura\n",
        "metro.head()"
      ],
      "metadata": {
        "id": "qUViTSj2EPy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 48204 linhas = 48204 horas registradas\n",
        "# 9 colunas\n",
        "# date_time nos dá diversos detalhes em apenas uma coluna \n",
        "# apesar de estar em object, podemos converte-lo na propria leitura\n",
        "metro.shape"
      ],
      "metadata": {
        "id": "ztm5qUBM9bet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metro['holiday'].unique())\n",
        "print(metro['weather_main'].unique())\n",
        "print(metro['weather_description'].unique())"
      ],
      "metadata": {
        "id": "fNaKQYh7FeTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mt = metro.copy()\n",
        "categorias = {}\n",
        "for col in mt.columns:\n",
        "  if mt[col] \n",
        "    categorias[col] = tipos\n",
        "print(categorias)"
      ],
      "metadata": {
        "id": "1l0Ti2gD81En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mt"
      ],
      "metadata": {
        "id": "xTt5PeyJ_CED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mt['hora'] = mt['date_time'].dt.hour() # dt converte de numpy para um tipo python\n",
        "mt['diaSemana'] = mt['date_time'].dt.weekday()\n",
        "mt['semanaAno'] = mt['date_time'].dt.isocalendar().week  \n",
        "mt = mt.drop('date_time', axis=1)"
      ],
      "metadata": {
        "id": "NGUoa5Ut9-Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mt"
      ],
      "metadata": {
        "id": "7Bp-GOD0_PxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = mt.drop('traffic_volume', axis=1).values\n",
        "# y = "
      ],
      "metadata": {
        "id": "D_FgNqcs_Us7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}